{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "### An artificial example to demonstrate feature selection based on maximizing validation $R^2$, rather than p-values.\n",
    "\n",
    "If the dimension of feature space is high w.r.t. number of observations:\n",
    "- become too specifically adjusted to the training set (overfitting), which would reduce its generalizeability (performance on the validation/test set);\\\\\n",
    "- lead to multicollinearity of regressors and high variance in their estimates, making coefficients hard to interpret.\n",
    "\n",
    "\n",
    "\n",
    "Reducing complexity of the model (number of features or their dimensionality) is usually recommended in such cases. The most streighforward way of doing so is through feature selection. \n",
    "\n",
    "\n",
    "In the beginning, we show a simple way of feature selection by p value.\n",
    "\n",
    "Then, select a subset of the regressors of the given size $k$ maximizing the model fit ($R^2$).\n",
    "Step-forward\n",
    "Step-backward\n",
    "\n",
    "Let's remind you:\n",
    "\n",
    "\n",
    "The regular $R^2$:\n",
    "$$\n",
    "R^2=1-\\frac{RSS}{\\sum_i (y_i-\\bar{y})^2}=\\frac{\\sum_i (\\hat{y}_i-\\bar{y})^2}{\\sum_i (y_i-\\bar{y})^2},\n",
    "$$\n",
    "The adjusted $R^2$:\n",
    "$$\n",
    "adj.R^2=1-\\frac{RSS}{\\sum_i (y_i-\\bar{y})^2} \\cdot \\frac{N-1}{N-K-1} \n",
    "$$\n",
    "where $N$ is the total sample size and $K$ is the number of features/selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import t\n",
    "n = 15\n",
    "np.random.seed(1418)\n",
    "\n",
    "col = []\n",
    "for i in range(n):\n",
    "    col.append('x%d'%(i+1))\n",
    "\n",
    "X = pd.DataFrame(np.random.randn(50*n).reshape(50,n))\n",
    "Y = X.ix[:,:3].sum(axis = 1) + np.random.randn(50)\n",
    "data3 = pd.concat((Y,X), axis = 1)  # concatenate Y and X\n",
    "data3.columns = ['Y'] + col         # rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The regression function based on the feature selection\n",
    "def AdjR2(flag):\n",
    "    '''\n",
    "    Evalute the model with adjusted R-squared,\n",
    "    with flag indicating the feature selection\n",
    "    1--feature included\n",
    "    0--feature excluded\n",
    "    '''\n",
    "    formstr='Y~-1'\n",
    "    for i in range(len(flag)):\n",
    "        if flag[i]==1:\n",
    "            formstr += '+x%d'%(i+1) # Construct the regression formula based on flag\n",
    "    train = data3[:20]\n",
    "    valid = data3[20:]\n",
    "    lm = smf.ols(formula = formstr, data = train).fit()\n",
    "    lmy = lm.predict(valid)\n",
    "    y_err = lmy-valid.Y\n",
    "    y_norm = valid.Y-np.mean(valid.Y)\n",
    "    # Adjusted R^2\n",
    "    R2 = 1 - y_err.dot(y_err) / y_norm.dot(y_norm) * (n-1) / (n-sum(flag)-1)\n",
    "    return R2, lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AdjR2(np.array([1]+[0]*14))[0]\n",
    "#print AdjR2(np.array([1]+[0]*14))[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adjR2 = []\n",
    "regR2 = []\n",
    "for i in range(1, n):\n",
    "    a,b = AdjR2([1]*i + [0]*(n-i))\n",
    "    regR2.append(b.rsquared)\n",
    "    if a>0:\n",
    "        adjR2.append(a)\n",
    "    else:\n",
    "        adjR2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "# In sample R2\n",
    "plt.plot(range(1, n), regR2, 'b-*', markersize = 10,label=\"IS\")\n",
    "# Out of sample R2 (adjusted)\n",
    "plt.plot(range(1, n), adjR2, 'r-*', markersize = 10,label=\"OS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature selection based on the p-values\n",
    "def DropP(threshold = 1):\n",
    "    '''Drop all features with p-values higher than the threshold'''\n",
    "    flag_p = (AdjR2([1]*n)[1]).pvalues < threshold\n",
    "    print('Validation R^2 is %f'%AdjR2(flag_p)[0])\n",
    "    print(AdjR2(flag_p)[1].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First of all, let's try to include all features and observe their p-values\n",
    "Note that if we include them all, we have a nagative one (N-N-1) in the denominator (since N = K), \n",
    "$$\n",
    "adj.R^2=1-\\frac{RSS}{\\sum_i (y_i-\\bar{y})^2} \\cdot \\frac{N-1}{N-K-1} \n",
    "$$\n",
    "which simply flips the sign of the second term of the right hand side, making the adjusted $R^2$ ridiculously high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DropP(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we remove variables based on their p-value (if > 0.2), then we'll keep $x_1,x_3,x_4,x_9,x_{15}$, and the model's out of sample $R^2$ is 0.097692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DropP(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we try to select feature in a fashion that maximizing validation adjusted $R^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the best feature selection that maximizes the out-of-sample adjusted R2\n",
    "def FindBestI():\n",
    "    flag = np.zeros(n)    # initially exclude all regressors: 1-include, 0-exclude\n",
    "    r2max = 0             # for storing and compare with best R^2\n",
    "    while True:\n",
    "        flag_mat = np.maximum(np.eye(n),flag)\n",
    "        # see if R2 increases if we add one more feature\n",
    "        r2 = np.apply_along_axis(AdjR2,1,flag_mat)[:,0]  # 1: row-wise operation, then select R2 only\n",
    "        temp = r2.max()\n",
    "        if temp > r2max:\n",
    "            r2max = temp\n",
    "            flag = flag_mat[r2.argmax()]  # select this feature if it improves our R2\n",
    "        else:\n",
    "            break                         # if there's nothing to add, break the loop\n",
    "    return flag\n",
    "\n",
    "flag = FindBestI()             # store the best choice of feature selection\n",
    "r2_best,lm_best = AdjR2(flag)  # then run the regression based on such selection\n",
    "ix = [col[i] for i in range(n) if flag[i]==1]\n",
    "\n",
    "b = pd.DataFrame({'Pvalues':AdjR2([1]*n)[1].pvalues, 'Keeper':col})\n",
    "c = 'forestgreen'\n",
    "def highlight_pval(val):\n",
    "    color = c if val in ix else 'white'\n",
    "    return 'background-color: %s'%color\n",
    "\n",
    "b.style.applymap(highlight_pval, subset=['Keeper']).\\\n",
    "    bar(subset = ['Pvalues'], color=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The length of green bar in right column indicates the p-value, and we can see that despite having high p-value,  $x_1$ is still included in our \"best fit\", which accurately reflects our true model that $y \\sim x_1 + x_2 + x_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Validation R^2 is %f'%r2_best)\n",
    "print(lm_best.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question: Maybe it is a special cut? We will see in next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Principle component analysis\n",
    "\n",
    "Feature selection is often too rigid - we have to make our choices of keeping or getting rid of each variable entirely, though it might be the case that each regressor by itself still contains certain valuable information, but all together the feature space is redundant. As an alternative one can think of expressing our big amount of regressors through some smaller amount of latent variables (other than initial regressors) able to explain all or almost all the relevant information. This is called dimensionality reduction.\n",
    "\n",
    "A most common linear approach to dimentionality reduction is the principal component analysis. An idea is that given $N$ observations for the $n$ regressors $x_j$\n",
    "\n",
    "$$\n",
    "X=\\{x_i^j, i=1..n, j=1..N\\}\n",
    "$$\n",
    "\n",
    "we try to come up with a linear transformation\n",
    "\n",
    "$$\n",
    "U=X V,\n",
    "$$\n",
    "\n",
    "where $V$ is $nxp$-dimensional transformation matrix and $U$ is a $N x p$ matrix of new factors $u_1,u_2,...,u_p$ (columns of $U$), such that they explain as much of initial information contained in $X$ as possible for the $p$ latent variables. \n",
    "\n",
    "## Technique\n",
    "\n",
    "Before applying PCA variables $x_i$ are usually centered ($E[x_i]=0$) and sometimes also normalized ($var[x_i]=1$).\n",
    "\n",
    "Selecting first principle component: look for the column unit basis $N x 1$ vector of weights/loadings $v_1$, such that resulting variable $u_1=X v_1$ has maximal possible variance $var[u_1]$:\n",
    "\n",
    "$$\n",
    "v_1=argmax_{v_1: v_1^T v_1=1}var[u_1] =argmax_{v_1: v_1^T v_1=1}u_1^T u_1=argmax_{v_1: v_1^T v_1=1}v_1^T X^T X v_1.\n",
    "$$\n",
    "\n",
    "The after first basis vector $v_1$ and principle component $u_1$ are selected, the second basis vector $v_2$ is selected following the same principle, but with an additional constrain of being orthogonal to $v_1$. \n",
    "\n",
    "Similarly each following $i$-th vector $v_i$ is being defined as\n",
    "$$\n",
    "v_i=argmax_{v_i: v_i^T v_i=1, v_i^T v_j=0, j<i}v_i^T X^T X v_i.\n",
    "$$\n",
    "\n",
    "The problem is easy to solve knowing the eigenvectors of $X^T X$, i.e. such unit vectors $v_i$ ($v_i^T v_i=1$) that\n",
    "\n",
    "$$\n",
    "\\lambda_i v_i=X^T X v_i\n",
    "$$\n",
    "\n",
    "or in the matrix form\n",
    "\n",
    "$$\n",
    "diag(\\lambda)V=X^T X V\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ are the corresponding eigenvalues (mutually distinctive). First of all its easy to see that such vectors $v_i$ are always mutually orthogonal, as\n",
    "\n",
    "$$\n",
    "v_j^T v_i= v_j^T X^T X v_i/\\lambda_i =(X^T X w_j)^T v_i/\\lambda_i=v_j^T v_i \\lambda_j/\\lambda_i.\n",
    "$$\n",
    "\n",
    "This is why $V^T V=I_n$.\n",
    "\n",
    "Now if we know $n$ unit eigenvectors with distinct eigenvalues such that $\\lambda_1>\\lambda_2> ...>\\lambda_n>0$, they produce an orthogonal basis in the $n$-dimensional space and for any unit vector $w=e_1 v_1+e_2 v_2+... +e_n v_n$ we can see that\n",
    "\n",
    "$$\n",
    "w^T X^T X w=\\lambda_1 e_1^2+\\lambda_2 e_2^2+...+\\lambda_n e_n^2,\n",
    "$$\n",
    "\n",
    "which is maximized for $e_1=1, e_2=e_3=...e_n=0$ (as $\\sum_i e_i^2=1$), i.e. $w=v_1$. \n",
    "\n",
    "Similarly the second, third, etc loading vectors could be found as $v_2,v_3,...$ correspondingly.\n",
    "\n",
    "Also the solution follows from the singular value decomposition of the matrix $X$:\n",
    "\n",
    "$$\n",
    "X=W \\Sigma V^T,\n",
    "$$\n",
    "\n",
    "where $W$ is a $N x n$ matrix of mutually orthogonal unit columns, $V$ is a $n x n$ matrix of mutually orthogonal unit columns and $\\Sigma$ is an $n x n$- diagonal matrix, i.e.\n",
    "\n",
    "$$\n",
    "W^T W=V^T V=I_n\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "X^T X=V\\Sigma W^T W \\Sigma V^T=V\\Sigma^2 V^T\n",
    "$$\n",
    "\n",
    "and its easy to see that $V$ is the matrix of eigenvectors and $\\Sigma$ is the diagonal matrix of square roots of eigenvalues. Then \n",
    "\n",
    "$$\n",
    "U=XV=W\\Sigma V^T V=W\\Sigma.\n",
    "$$\n",
    "\n",
    "## PCA as a dimensionality reduction tool\n",
    "\n",
    "Now once matrixes $V, U, \\Sigma$ are defined as well as the eigenvalues $\\lambda_j$ (being the squares of the diagonal elements of $\\Sigma$), how do we use that for dimensionality reduction? \n",
    "\n",
    "Matrix $V$ performs a transformation of the regressors $x_i$ to the new orthogonal variables $u_i$ being the columns of $U$. And \n",
    "$$\n",
    "Var[u_i]=\\lambda_i\n",
    "$$\n",
    "\n",
    "This is interpreted as each variable $u_i$ containing the fraction $\\lambda_i/\\sum\\limits_j \\lambda_j$ of the entire information (variation) contained in all the regressors. And the choice of regressors $u_i$ is such that $\\lambda_1,\\lambda_2,...$ are sequentially maximized. So if one wants to select as few latent variables as possible in order to cover a given franction $\\alpha$ (often $95\\%$) of information (variation) from it is enough to select first $k$ principle components $u_1,u_2,...u_k$, so that \n",
    "\n",
    "$$\n",
    "\\frac{\\sum\\limits_{i=1}^k\\lambda_i}{\\sum\\limits_{i=1}^n\\lambda_i}\\geq \\alpha.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import modules we need to use for this session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# PCA module\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Toy example to understand PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) create some data and plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
    "plt.plot(X[:, 0], X[:, 1], 'o')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) principal components\n",
    "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"total variance:{}\".format(np.sum(np.var(X,0))))\n",
    "pca = PCA(2)\n",
    "pca.fit(X)\n",
    "print(\"variance explained via the first and second components:{}\\n\".format(pca.explained_variance_))\n",
    "print(\"principal components:\\n{}\".format(pca.components_))\n",
    "#pca.explained_variance_ratio_\n",
    "#np.var(X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) let's plot the two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4)let's project the data onto the first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = PCA(0.95) # keep 95% of variance. This is another way to define the hyperparameters.\n",
    "X_trans = clf.fit_transform(X)\n",
    "X_new = clf.inverse_transform(X_trans) # transformed data\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) let's project the data onto the second principal component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e2=pca.components_[1,:] # this is the 2nd eigenvector\n",
    "X_new2=(np.dot(X,e2)*e2.reshape((2,1))).T  # this is the data projected onto the 2nd eigenvector\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)\n",
    "plt.plot(X_new2[:, 0], X_new2[:, 1], 'ob', alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick example to show dimension reduction by PCA \n",
    "\n",
    "Iris dataset:\n",
    "https://en.wikipedia.org/wiki/Iris_flower_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Sepal_lenth</th>\n",
       "      <th>Sepal_width</th>\n",
       "      <th>Petal_length</th>\n",
       "      <th>Petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Species  Sepal_lenth  Sepal_width  Petal_length  Petal_width\n",
       "0      0.0          5.1          3.5           1.4          0.2\n",
       "1      0.0          4.9          3.0           1.4          0.2\n",
       "2      0.0          4.7          3.2           1.3          0.2\n",
       "3      0.0          4.6          3.1           1.5          0.2\n",
       "4      0.0          5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # we only take the first two features.\n",
    "Y = iris.target\n",
    "data_i=pd.DataFrame(np.concatenate((Y.reshape(len(Y),1),X),axis=1))\n",
    "data_i.columns=[\"Species\",\"Sepal_lenth\",\"Sepal_width\",\"Petal_length\",\"Petal_width\"]\n",
    "data_i.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg\",width=100,height=100>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\",width=100,height=100>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg\",width=100,height=100>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now what if we want to visulize the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Covariance between X.\n",
    "pd.DataFrame(X).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=4 # how many eigenvectors we choose\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n)\n",
    "Xproj = pca.fit_transform(X)\n",
    "eigenvalues = pca.explained_variance_\n",
    "print pca.explained_variance_ratio_\n",
    "plt.bar(np.arange(n), eigenvalues);\n",
    "plt.xlabel(\"Dimensionality\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Reduce the dimension from 4 to 2 and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "# Plot the training points\n",
    "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=Y, cmap=plt.cm.cool)\n",
    "plt.xlabel('First Eigenvalue')\n",
    "plt.ylabel('Second Eigenvalue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dimension from 4 to 3 and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(Xproj[:, 0], Xproj[:, 1], Xproj[:, 2], c=Y,\n",
    "           cmap=plt.cm.cool)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Artificial data (Using PCA as feature selection tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"https://serv.cusp.nyu.edu/~lw1474/ADS_Data/session06/data_q1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.035437</td>\n",
       "      <td>-18.227424</td>\n",
       "      <td>30.521149</td>\n",
       "      <td>-37.494755</td>\n",
       "      <td>31.108940</td>\n",
       "      <td>-8.917930</td>\n",
       "      <td>13.177162</td>\n",
       "      <td>10.153335</td>\n",
       "      <td>17.521327</td>\n",
       "      <td>14.695400</td>\n",
       "      <td>...</td>\n",
       "      <td>11.332401</td>\n",
       "      <td>-3.530624</td>\n",
       "      <td>14.702212</td>\n",
       "      <td>14.099834</td>\n",
       "      <td>4.037464</td>\n",
       "      <td>12.604090</td>\n",
       "      <td>4.788002</td>\n",
       "      <td>9.592441</td>\n",
       "      <td>7.887213</td>\n",
       "      <td>9.061646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.479243</td>\n",
       "      <td>31.210052</td>\n",
       "      <td>8.273238</td>\n",
       "      <td>17.545475</td>\n",
       "      <td>8.013616</td>\n",
       "      <td>-3.163515</td>\n",
       "      <td>12.632948</td>\n",
       "      <td>10.384905</td>\n",
       "      <td>7.278064</td>\n",
       "      <td>11.506060</td>\n",
       "      <td>...</td>\n",
       "      <td>5.867968</td>\n",
       "      <td>6.762153</td>\n",
       "      <td>11.128908</td>\n",
       "      <td>6.466586</td>\n",
       "      <td>10.245606</td>\n",
       "      <td>10.736484</td>\n",
       "      <td>5.349025</td>\n",
       "      <td>21.841173</td>\n",
       "      <td>-3.204046</td>\n",
       "      <td>19.017705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.193325</td>\n",
       "      <td>-2.427634</td>\n",
       "      <td>1.698858</td>\n",
       "      <td>-1.980364</td>\n",
       "      <td>-9.902694</td>\n",
       "      <td>9.215057</td>\n",
       "      <td>9.984092</td>\n",
       "      <td>7.627307</td>\n",
       "      <td>7.701929</td>\n",
       "      <td>15.254064</td>\n",
       "      <td>...</td>\n",
       "      <td>15.241884</td>\n",
       "      <td>4.876780</td>\n",
       "      <td>13.953317</td>\n",
       "      <td>11.608533</td>\n",
       "      <td>7.515592</td>\n",
       "      <td>3.413471</td>\n",
       "      <td>12.893463</td>\n",
       "      <td>1.633385</td>\n",
       "      <td>21.345764</td>\n",
       "      <td>7.264307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          y          0          1          2          3         4          5  \\\n",
       "0  3.035437 -18.227424  30.521149 -37.494755  31.108940 -8.917930  13.177162   \n",
       "1  1.479243  31.210052   8.273238  17.545475   8.013616 -3.163515  12.632948   \n",
       "2 -1.193325  -2.427634   1.698858  -1.980364  -9.902694  9.215057   9.984092   \n",
       "\n",
       "           6          7          8    ...             15        16         17  \\\n",
       "0  10.153335  17.521327  14.695400    ...      11.332401 -3.530624  14.702212   \n",
       "1  10.384905   7.278064  11.506060    ...       5.867968  6.762153  11.128908   \n",
       "2   7.627307   7.701929  15.254064    ...      15.241884  4.876780  13.953317   \n",
       "\n",
       "          18         19         20         21         22         23         24  \n",
       "0  14.099834   4.037464  12.604090   4.788002   9.592441   7.887213   9.061646  \n",
       "1   6.466586  10.245606  10.736484   5.349025  21.841173  -3.204046  19.017705  \n",
       "2  11.608533   7.515592   3.413471  12.893463   1.633385  21.345764   7.264307  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=data.iloc[:,1:]\n",
    "y=np.asarray(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1. What the corr matrix\n",
    "# x.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. Multivariable regression. y~X (No intercept).\n",
    "\n",
    "#(1) sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "ols=LinearRegression(fit_intercept=False)\n",
    "ols.fit(x,y)\n",
    "print ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(2) statsmodels for the same y~x. \n",
    "import statsmodels.api as sm\n",
    "res=sm.OLS(y,x).fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The R-squared looks very good?? (Before we go ahead, let's take a quick look at feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### feature selection by p-values.\n",
    "res=sm.OLS(y,x.iloc[:,[7,13,18]]).fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using cross validation.\n",
    "R_IS=[]\n",
    "R_OS=[]\n",
    "\n",
    "# Why we need this? let's set n=1 and run several times to see.\n",
    "n=1000\n",
    "from sklearn.cross_validation import train_test_split\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n",
    "    \n",
    "    res=LinearRegression(fit_intercept=False)\n",
    "    res.fit(X_train,y_train)\n",
    "    R_IS.append(1-((np.asarray(res.predict(X_train))-y_train)**2).sum()/((y_train-np.mean(y_train))**2).sum())                                                                     \n",
    "    R_OS.append(1-((np.asarray(res.predict(X_test))-y_test)**2).sum()/((y_test-np.mean(y_test))**2).sum())\n",
    "print(\"IS R-squared for {} times is {}\".format(n,np.mean(R_IS)))\n",
    "print(\"OS R-squared for {} times is {}\".format(n,np.mean(R_OS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using cross validation after feature selection.\n",
    "R_IS=[]\n",
    "R_OS=[]\n",
    "n=1000\n",
    "from sklearn.cross_validation import train_test_split\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.asarray(x.iloc[:,[7,13,18]]), y, test_size=0.33)\n",
    "    res=LinearRegression(fit_intercept=False)\n",
    "    res.fit(X_train,y_train)\n",
    "    R_IS.append(1-((np.asarray(res.predict(X_train))-y_train)**2).sum()/((y_train-np.mean(y_train))**2).sum())                                                                     \n",
    "    R_OS.append(1-((np.asarray(res.predict(X_test))-y_test)**2).sum()/((y_test-np.mean(y_test))**2).sum())\n",
    "print(\"IS R-squared for {} times is {}\".format(n,np.mean(R_IS)))\n",
    "print(\"OS R-squared for {} times is {}\".format(n,np.mean(R_OS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=25 # how many eigenvectors we choose\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n)\n",
    "Xproj = pca.fit_transform(x)\n",
    "eigenvalues = pca.explained_variance_ratio_\n",
    "plt.plot(np.arange(n), eigenvalues, 'o')\n",
    "plt.xlabel(\"Dimensionality\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Correlation between variabls in Xproj after transformation.\n",
    "# pd.DataFrame(Xproj).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "res=sm.OLS(y,pd.DataFrame(Xproj)).fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What about the feature selection this time?\n",
    "import statsmodels.api as sm\n",
    "res=sm.OLS(y,pd.DataFrame(Xproj).iloc[:,[0,1,3]]).fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using cross validation.Looks much better compared to first one if you remember.\n",
    "R_IS=[]\n",
    "R_OS=[]\n",
    "n=1000\n",
    "from sklearn.cross_validation import train_test_split\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xproj[:,[0,1,3]], y, test_size=0.4)\n",
    "    res=LinearRegression(fit_intercept=False)\n",
    "    res.fit(X_train,y_train)\n",
    "    R_IS.append(1-((np.asarray(res.predict(X_train))-y_train)**2).sum()/((y_train-np.mean(y_train))**2).sum())                                                                     \n",
    "    R_OS.append(1-((np.asarray(res.predict(X_test))-y_test)**2).sum()/((y_test-np.mean(y_test))**2).sum())\n",
    "print(\"IS R-squared for {} times is {}\".format(n,np.mean(R_IS)))\n",
    "print(\"OS R-squared for {} times is {}\".format(n,np.mean(R_OS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Example\n",
    "\n",
    "Before we tried to use some parameters of the house (size) to predict its price. However notice that location matters as well. Below we upload a sample (approximately 30% of the total) of more than 2000 individual house sales all around NYC in 2012. Each record together with the parameters of the house also contains important characteristics of the location (zip code) - average income of its residents (accodring to US census), as well as the relative structure of 311 complaints happening in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>gross_sq_feet</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11356</td>\n",
       "      <td>600000</td>\n",
       "      <td>1624</td>\n",
       "      <td>80098.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10017</td>\n",
       "      <td>5200000</td>\n",
       "      <td>3840</td>\n",
       "      <td>149723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11413</td>\n",
       "      <td>100000</td>\n",
       "      <td>2120</td>\n",
       "      <td>84085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11213</td>\n",
       "      <td>775000</td>\n",
       "      <td>3423</td>\n",
       "      <td>46614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11207</td>\n",
       "      <td>151000</td>\n",
       "      <td>2136</td>\n",
       "      <td>44634.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zip_code  sale_price  gross_sq_feet      mean\n",
       "0     11356      600000           1624   80098.0\n",
       "1     10017     5200000           3840  149723.0\n",
       "2     11413      100000           2120   84085.0\n",
       "3     11213      775000           3423   46614.0\n",
       "4     11207      151000           2136   44634.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4=pd.read_csv(\"https://serv.cusp.nyu.edu/~lw1474/ADS_Data/session06/example4.csv\").iloc[:,2:]\n",
    "data4.loc[:,[\"zip_code\",\"sale_price\",\"gross_sq_feet\",\"mean\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run a linear regression: \"sale_price\"~\"gross_sq_feet\"+\"mean\"\n",
    "res=sm.OLS(data4.sale_price,sm.add_constant(data4.loc[:,[\"gross_sq_feet\",\"mean\"]]),missing=\"drop\").fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#If using sklearn.\n",
    "lm=LinearRegression()\n",
    "lm.fit(data4.loc[:,[\"gross_sq_feet\",\"mean\"]],data4.sale_price)\n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using cross validation to show average R-squared\n",
    "R_IS=[]\n",
    "R_OS=[]\n",
    "n=100\n",
    "from sklearn.cross_validation import train_test_split\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data4.loc[:,[\"gross_sq_feet\",\"mean\"]],data4.sale_price,test_size=0.3)\n",
    "    res=LinearRegression(fit_intercept=False)\n",
    "    res.fit(X_train,y_train)\n",
    "    R_IS.append(1-((np.asarray(res.predict(X_train))-y_train)**2).sum()/((y_train-np.mean(y_train))**2).sum())                                                                     \n",
    "    R_OS.append(1-((np.asarray(res.predict(X_test))-y_test)**2).sum()/((y_test-np.mean(y_test))**2).sum())\n",
    "print(\"IS R-squared for {} times is {}\".format(n,np.mean(R_IS)))\n",
    "print(\"OS R-squared for {} times is {}\".format(n,np.mean(R_OS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: How to increase average OS R-squared to get a better prediction?\n",
    "#### Q2: How to get a better estimated coefficient for sale_price~gross_sq_feet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all 311 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 311 service requests data.\n",
    "https://en.wikipedia.org/wiki/3-1-1\n",
    "\n",
    "3-1-1 is a special telephone number supported in many communities in Canada and the United States. The number provides access to non-emergency municipal services. The number format follows the N11 code for a group of short, special-purpose local numbers as designated in the North American Numbering Plan.\n",
    "The number 3-1-1 is intended in part to divert routine inquiries and non-urgent community concerns from the 9-1-1 number which is reserved for emergency service. A promotional website for 3-1-1 in Akron described the distinction as follows: \"Burning building? Call 9-1-1. Burning Question? Call 3-1-1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_311=list(data4.loc[:,\"Adopt A Basket\":\"X Ray Machine Equipment\"].columns)\n",
    "data5=data4[[\"sale_price\",\"gross_sq_feet\",\"mean\"]+list_311]\n",
    "#list_311\n",
    "#data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run \"sale_price\"~\"gross_sq_feet\"+\"mean\"+\"all 311\" What is new R-squared? Remove constant.\n",
    "res=sm.OLS(data4.sale_price,sm.add_constant(data4.loc[:,[\"gross_sq_feet\",\"mean\"]+list_311]),missing=\"drop\").fit()\n",
    "\n",
    "# Take a look p values if you are interested by following code\n",
    "#print res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Over-fitting problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step one: Split the data to training and testing data: Using \"train_test_split\", and set random_state=324 and test_size=0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(data5.loc[:,\"gross_sq_feet\":],data5.sale_price,random_state=324,test_size=0.3)\n",
    "\n",
    "#Step two: train the model again using training data.\n",
    "\n",
    "res=sm.OLS(y_train,sm.add_constant(X_train),missing=\"drop\").fit()\n",
    "# print res.rsquared()\n",
    "print(\"IS-R-squared:{}\".format(res.rsquared))\n",
    "\n",
    "res=LinearRegression(fit_intercept=False)\n",
    "res.fit(X_train,y_train)\n",
    "print(\"IS-R-squared:{}\".format(1-((np.asarray(res.predict(X_train))-y_train)**2).sum()/((y_train-np.mean(y_train))**2).sum()))                                                                   \n",
    "\n",
    "#Step three: Get the out of sample prediction, and calculate Out of sample R-squared, and compare it to insample R-squared.\n",
    "print(\"OS-R-squared:{}\".format(1-((np.asarray(res.predict(X_test))-y_test)**2).sum()/((y_test-np.mean(y_test))**2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step four: try 100 times random split the data and get the average OS and IS R-squared\n",
    "R_IS=[]\n",
    "R_OS=[]\n",
    "n=100\n",
    "from sklearn.cross_validation import train_test_split\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data5.loc[:,\"gross_sq_feet\":],data5.sale_price,test_size=0.3)\n",
    "    res=LinearRegression(fit_intercept=False)\n",
    "    res.fit(X_train,y_train)\n",
    "    R_IS.append(1-((np.asarray(res.predict(X_train))-y_train)**2).sum()/((y_train-np.mean(y_train))**2).sum())                                                                     \n",
    "    R_OS.append(1-((np.asarray(res.predict(X_test))-y_test)**2).sum()/((y_test-np.mean(y_test))**2).sum())\n",
    "print(\"IS R-squared for {} times is {}\".format(n,np.mean(R_IS)))\n",
    "print(\"OS R-squared for {} times is {}\".format(n,np.mean(R_OS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's using PCA on 311 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adopt A Basket</th>\n",
       "      <th>Air Quality</th>\n",
       "      <th>Animal Abuse</th>\n",
       "      <th>Animal Facility   No Permit</th>\n",
       "      <th>Animal in a Park</th>\n",
       "      <th>APPLIANCE</th>\n",
       "      <th>Asbestos</th>\n",
       "      <th>Beach Pool Sauna Complaint</th>\n",
       "      <th>BEST Site Safety</th>\n",
       "      <th>Bike Rack Condition</th>\n",
       "      <th>...</th>\n",
       "      <th>Unsanitary Pigeon Condition</th>\n",
       "      <th>Urinating in Public</th>\n",
       "      <th>Vacant Lot</th>\n",
       "      <th>Vending</th>\n",
       "      <th>Violation of Park Rules</th>\n",
       "      <th>Water Conservation</th>\n",
       "      <th>Water Quality</th>\n",
       "      <th>Water System</th>\n",
       "      <th>Window Guard</th>\n",
       "      <th>X Ray Machine Equipment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.049281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.050874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.007354</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.069058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.012433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.005897</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.037445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Adopt A Basket  Air Quality  Animal Abuse  Animal Facility   No Permit  \\\n",
       "0        0.000000     0.027721           0.0                     0.000000   \n",
       "1        0.000000     0.010675           0.0                     0.000000   \n",
       "2        0.000000     0.002511           0.0                     0.000000   \n",
       "3        0.000000     0.001476           0.0                     0.000000   \n",
       "4        0.000049     0.001671           0.0                     0.000049   \n",
       "\n",
       "   Animal in a Park  APPLIANCE  Asbestos  Beach Pool Sauna Complaint  \\\n",
       "0          0.000770   0.000257  0.001027                    0.001797   \n",
       "1          0.000227   0.000227  0.003861                    0.000000   \n",
       "2          0.001076   0.001973  0.000359                    0.000000   \n",
       "3          0.000170   0.006756  0.000738                    0.000057   \n",
       "4          0.000344   0.006143  0.000688                    0.000000   \n",
       "\n",
       "   BEST Site Safety  Bike Rack Condition           ...             \\\n",
       "0          0.001540             0.000000           ...              \n",
       "1          0.001363             0.000000           ...              \n",
       "2          0.000359             0.000000           ...              \n",
       "3          0.000227             0.000114           ...              \n",
       "4          0.000541             0.000049           ...              \n",
       "\n",
       "   Unsanitary Pigeon Condition  Urinating in Public  Vacant Lot   Vending  \\\n",
       "0                     0.000000             0.000000    0.000000  0.001797   \n",
       "1                     0.000227             0.000227    0.000000  0.010675   \n",
       "2                     0.000000             0.000000    0.004484  0.000000   \n",
       "3                     0.000170             0.000170    0.000908  0.000568   \n",
       "4                     0.000491             0.000197    0.005897  0.000590   \n",
       "\n",
       "   Violation of Park Rules  Water Conservation  Water Quality  Water System  \\\n",
       "0                 0.001540            0.003850       0.002053      0.049281   \n",
       "1                 0.000454            0.001817       0.000227      0.050874   \n",
       "2                 0.000179            0.007354       0.001256      0.069058   \n",
       "3                 0.000170            0.000454       0.000341      0.012433   \n",
       "4                 0.000393            0.001622       0.000442      0.037445   \n",
       "\n",
       "   Window Guard  X Ray Machine Equipment  \n",
       "0           0.0                      0.0  \n",
       "1           0.0                      0.0  \n",
       "2           0.0                      0.0  \n",
       "3           0.0                      0.0  \n",
       "4           0.0                      0.0  \n",
       "\n",
       "[5 rows x 159 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6=data5.loc[:,\"Adopt A Basket\":]\n",
    "data6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the correlation matrix for data6.\n",
    "#data6.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the biggest 30 eigenvalues.\n",
    "n=30\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n)\n",
    "Xproj = pca.fit_transform(data6)\n",
    "eigenvalues = pca.explained_variance_ratio_\n",
    "plt.plot(np.arange(n), eigenvalues, 'o')\n",
    "plt.xlabel(\"Dimensionality\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform data6 to the top 6 eigenspace and merge the new data back to \"gross_sq_feet\" and \"mean\" and run the regression again.\n",
    "X=pd.concat((data5[[\"gross_sq_feet\",\"mean\"]],pd.DataFrame(Xproj[:,:6])),axis=1)\n",
    "y=data5[\"sale_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Look at the linear regression result\n",
    "#X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1=sm.OLS(y,sm.add_constant(X),missing=\"drop\").fit()\n",
    "print res1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_IS=[]\n",
    "R_OS=[]\n",
    "n=100\n",
    "from sklearn.cross_validation import train_test_split\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4)\n",
    "    res=LinearRegression(fit_intercept=False)\n",
    "    res.fit(X_train,y_train)\n",
    "    R_IS.append(1-((np.asarray(res.predict(X_train))-y_train)**2).sum()/((y_train-np.mean(y_train))**2).sum())                                                                     \n",
    "    R_OS.append(1-((np.asarray(res.predict(X_test))-y_test)**2).sum()/((y_test-np.mean(y_test))**2).sum())\n",
    "print(\"IS R-squared for {} times is {}\".format(n,np.mean(R_IS)))\n",
    "print(\"OS R-squared for {} times is {}\".format(n,np.mean(R_OS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we want explain sale_price~gross_sq_feet in a more convincing way? (Let's do it if we have time left)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
