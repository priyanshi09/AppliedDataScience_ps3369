{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "What if we do not have any training data? Unsupervised approach - k-means clustering can help learning the most suitable cluster structure given a suggested number of clusters $k$. \n",
    "\n",
    "Works for the data samples ($N$ points) in $n$-dimensional Euclidean space: \n",
    "$$\n",
    "X=\\{x_i, i=1..N\\}=\\{x_i^j, i=1..N, j=1..n\\},\n",
    "$$ \n",
    "where $x_i$ are the $n$-dimensional raw-vectors corresponding to each of the $N$ sample points. The approach is trying to infer cluster indexes $c_i$ (taking one of the possible values $1,2,...,k$) for each point $x_i$ in such a way that the cumulative squared distance from all the sample points $x_i$ to the centroids $\\mu_{c_i}$ of the corresponding clusters $c_i$ is minimized. This is quite an intuitive criteria, meaning that the clusters as as compact as possible for the given number $k$. The cluster centroids are defined as simple vector means\n",
    "$$\n",
    "\\mu_c=\\sum_{i, c(i)=c} x_i/m(c),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "m(c)=|\\{i, c(i)=c\\}|\n",
    "$$\n",
    "denotes the cluster size. Or in the coordinate form $\\mu_c=(\\mu_c^j, j=1..n)$\n",
    "$$\n",
    "\\mu_c^j=\\frac{\\sum_{i, c(i)=c} x_i^j}{m(c)}, j=1..n.\n",
    "$$\n",
    "Then the mimimal cumulatize squared distance criteria can be written as\n",
    "$$\n",
    "SD=\\sum_i \\|x_i-\\mu_{c_i}\\|^2=\\sum_{i,j} \\left(x_i^j-\\mu_{c_i}^j\\right)^2\\to \\min\n",
    "$$\n",
    "\n",
    "The common algorithm (often called after Lloyd) starts from a random cluster assignment and iterates the following two steps:\n",
    "\n",
    "A. Compute cluster means.\n",
    "\n",
    "B. Re-assign each point to the cluster with the mean closest to the considered point.\n",
    "\n",
    "Alternative approach is to initially use $k$ random points of the sample as clusters means, starting from the step B.\n",
    "\n",
    "The algorithm stops once a new iteration fails to alter any single cluster assignment.\n",
    "\n",
    "Algorithm always converges to a local optimum of $SD$, however there is no guarantee that this partitioning is indeed the global optimum. Also the final outcome happens to depend on the initial partitioning. This way it makes sense to repreat $k$-means a number of times (like $10,25...100$ etc) with different random initial partitioning, picking up the partitioning with the best final $SD$. \n",
    "\n",
    "In order to figure out how many trials to consider one can keep adding them one by one tracking how do they affect the final score and stop once it stays stable for a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib.pylab import plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features=2, centers=2,n_samples=20,random_state=9)\n",
    "plt.scatter(X[:,0],X[:,1],c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we calculate by hand for this simple example, we do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cent=np.asarray([[-6,-6],[-2,-1]])\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(1,4,1)\n",
    "plt.scatter(X[:,0],X[:,1],c=\"r\")\n",
    "for i in range(3):\n",
    "    plt.subplot(1,4,i+2)\n",
    "    y_t=np.sum((X-cent[0,:])**2,axis=1)**0.5<np.sum((X-cent[1,:])**2,axis=1)**0.5\n",
    "    plt.scatter(X[:,0],X[:,1],c=y_t,cmap=plt.cm.cool)\n",
    "    plt.scatter(cent[0,0],cent[0,1],c=\"r\",s=100)\n",
    "    plt.scatter(cent[1,0],cent[1,1],c=\"b\",s=100)\n",
    "\n",
    "    cent=np.asarray([[np.sum(X[:,0]*y_t)/(np.sum(y_t)),np.sum(X[:,1]*y_t)/np.sum(y_t)],\n",
    "            [np.sum(X[:,0]*(1-y_t))/np.sum(1-y_t),np.sum(X[:,1]*(1-y_t))/np.sum(1-y_t)]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Kmeans package from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features=2, centers=3,random_state=9999)\n",
    "plt.scatter(X[:,0],X[:,1],c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use package from sklearn.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n=2 # number of clusters\n",
    "dd=X #data\n",
    "tar=y # real target\n",
    "\n",
    "#train the model.\n",
    "km=KMeans(random_state=324,n_clusters=n)\n",
    "res=km.fit(dd)\n",
    "#result.\n",
    "print(res.labels_)\n",
    "# print res.predict(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.scatter(dd[:, 0], dd[:, 1], c=tar, cmap=plt.cm.cool)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(\"Real\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(dd[:, 0], dd[:, 1], c=res.labels_, cmap=plt.cm.cool)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(\"Kmeans-{} clusters\".format(n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe k=3 is more appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Choose the number of clusters. (k)\n",
    "\n",
    "#### Silhouette Coefficient\n",
    "In the example above we visually see that the data could still be clustered further. So how to select the appropriate number of clusters $k$ besides just naive visual observations? \n",
    "\n",
    "We can't use SD here anymore as the more clusters we take, the smaller it goes, and we'll get it down to zero, assigning each point to its own cluster, which is certianly not the most useful way of clustering.\n",
    "\n",
    "We need another partitioning quality measure here. Most common option is using a Silhouette measure, which for each data point $x_i$ quantifies its relative attachement strength to its current cluster vs the closest neighbor cluster:\n",
    "$$\n",
    "s(i)=\\frac{\\min\\limits_{k\\neq c_i} \\|x_i-\\mu_{c_k}\\|-\\|x_i-\\mu_{c_i}\\|}{\\max\\{\\|x_i-\\mu_{c_i}\\|,\\min\\limits_{k\\neq c_i} \\|x_i-\\mu_{c_k}\\|\\}}.\n",
    "$$\n",
    "\n",
    "Then internal quality of the partitioning is characterized by an average ratio value of silhouette for all the data points:\n",
    "$$\n",
    "S=\\frac{\\sum\\limits_i s(i)}{N}.\n",
    "$$\n",
    "\n",
    "By definition, the silhouette measure is normalized. i.e. $-1\\leq s(i)\\leq 1$ and so is the average: $-1\\leq S\\leq 1$. $S=1$ means that all the points coinside with their cluster centroids, values close to 1 mean that the points are usually much closer to their cluster centroids than to all the other ones (i.e. clustered appropriately), values of $S$ around zero mean that clustering is quite unstable, i.e. moving many of the points to another clusters might not affect its quality much; while negative values of $S$ mean that clustering could certainly be improved by moving some of the points to the other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "X=X\n",
    "range_n_clusters = [2, 3, 4, 5]\n",
    "for n_clusters in range_n_clusters:\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=324)\n",
    "    cluster_labels = km.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters ={},\".format(n_clusters)+\" the average silhouette_score is :{}\".format(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the result of silhouette_score, we should choose k=3. Let's see the plot.  s for each observation for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def silhouette_score_plot(data,range_n_clusters):\n",
    "    X=data\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(10, 5)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=324)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors)\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                    marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "silhouette_score_plot(X,range(2,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "The Elbow method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.\n",
    "\n",
    "First of all, compute the sum of squared error (SSE) for some values of k (for example 2, 4, 6, 8, etc.). The SSE is defined as the sum of the squared distance between each member of the cluster and its centroid. Mathematically:\n",
    "\n",
    "$SSE=\\sum^K_{i=1} \\sum_{x\\in c_i} dist(x,c_i)^2$\n",
    "\n",
    " If you plot k against the SSE, you will see that the error decreases as k gets larger; this is because when the number of clusters increases, they should be smaller, so distortion is also smaller. The idea of the elbow method is to choose the k at which the SSE decreases abruptly. This produces an \"elbow effect\" in the graph, as you can see in the following picture:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def elbow(data,K):\n",
    "#data is your input as numpy form\n",
    "#K is a list of number of clusters you would like to show.\n",
    "    # Run the KMeans model and save all the results for each number of clusters\n",
    "    KM = [KMeans(n_clusters=k).fit(data) for k in K]\n",
    "    \n",
    "    # Save the centroids for each model with a increasing k\n",
    "    centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "    # For each k, get the distance between the data with each center. \n",
    "    D_k = [cdist(data, cent, 'euclidean') for cent in centroids]\n",
    "    \n",
    "    # But we only need the distance to the nearest centroid since we only calculate dist(x,ci) for its own cluster.\n",
    "    globals()['dist'] = [np.min(D,axis=1) for D in D_k]\n",
    "    \n",
    "    # Calculate the Average SSE.\n",
    "    avgWithinSS = [sum(d)/data.shape[0] for d in dist]\n",
    "    \n",
    "    \n",
    "    # elbow curve\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, avgWithinSS, 'b*-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elbow for KMeans clustering')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Total with-in sum of square plot. Another way to show the result.\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    tss = sum(pdist(data)**2)/data.shape[0]\n",
    "    bss = tss-wcss\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, bss/tss*100, 'b*-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Percentage of variance explained')\n",
    "    plt.title('Elbow for KMeans clustering')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elbow(X, range(1,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Guassian Mixture. When KMeans does not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1,random_state=2)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X[:,0],X[:,1],c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Real vs KMeans vs Guassian Mixture\n",
    "n=2 # number of clusters\n",
    "dd=X #data\n",
    "tar=y # real target\n",
    "\n",
    "km=KMeans(n_clusters=n)\n",
    "res=km.fit(dd)\n",
    "#res.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you are using the recent sklearn: 0.18. (The old verison does not work correctly)\n",
    "\n",
    "If not, please go to the terminal and use the following code to upgrade it.\n",
    "\n",
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.dev0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Sklearn version.\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture # You can import this only if you are using 0.18+ sklearn.\n",
    "gm=GaussianMixture(n_components=n,random_state=324)\n",
    "res1=gm.fit(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot.\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(dd[:, 0], dd[:, 1], c=tar, cmap=plt.cm.cool)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(\"Real\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(dd[:, 0], dd[:, 1], c=res1.predict(dd), cmap=plt.cm.cool)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(\"Guassian Mixture\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(dd[:, 0], dd[:, 1], c=res.labels_, cmap=plt.cm.cool)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(\"Kmeans-{} clusters\".format(n))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world exercise. 2012 \"stop and frisk\" data\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stop-and-frisk_in_New_York_City\n",
    "\n",
    "The stop-question-and-frisk program, or stop-and-frisk, in New York City, is a practice of the New York City Police Department in which police officers stop and question a pedestrian, then frisk them for weapons and other contraband; this is what is known in other places in the United States as the Terry stop. The rules for stop, question, and frisk are found in the state's criminal procedure law section 140.50, and are based on the decision of the United States Supreme Court in the case of Terry v. Ohio.[1][2] About 685,724 people were stopped in 2011.[1][3][4] However, the number of stops has been reduced dramatically since then, to 22,939 in 2015.[3]\n",
    "\n",
    "The vast majority of those stopped were African-American or Latino.[1][3][4] According to a 2007 study, this disparity persists even after controlling for \"precinct variability and race-specific estimates of crime participation\".[5] Over half were aged 14-24. [3] In 2011 police stopped 46,784 women, frisking nearly 16,000. While women represented only 6.9% of stops in 2011, guns were found in only 59 cases; but there were 3,993 arrests of women. Guns were discovered on 0.12% and 0.13% of women and men respectively. [6]\n",
    "\n",
    "Stop-and-frisk has become an issue in the 2016 presidential election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_Friday</th>\n",
       "      <th>day_Monday</th>\n",
       "      <th>day_Saturday</th>\n",
       "      <th>day_Sunday</th>\n",
       "      <th>day_Thursday</th>\n",
       "      <th>day_Tuesday</th>\n",
       "      <th>day_Wednesday</th>\n",
       "      <th>month_April</th>\n",
       "      <th>month_August</th>\n",
       "      <th>month_December</th>\n",
       "      <th>...</th>\n",
       "      <th>month_November</th>\n",
       "      <th>month_October</th>\n",
       "      <th>month_September</th>\n",
       "      <th>time_1</th>\n",
       "      <th>time_2</th>\n",
       "      <th>time_3</th>\n",
       "      <th>time_4</th>\n",
       "      <th>time_5</th>\n",
       "      <th>time_6</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>10472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>10037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>10460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>11224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>979.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>885.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>...</td>\n",
       "      <td>395.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>2340.0</td>\n",
       "      <td>10456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   day_Friday  day_Monday  day_Saturday  day_Sunday  day_Thursday  \\\n",
       "0       278.0       212.0         315.0       213.0         254.0   \n",
       "1       199.0        79.0         301.0       154.0         163.0   \n",
       "2       278.0       169.0         283.0       203.0         234.0   \n",
       "3       175.0        82.0         177.0       176.0         180.0   \n",
       "4       979.0       647.0        1086.0       773.0         913.0   \n",
       "\n",
       "   day_Tuesday  day_Wednesday  month_April  month_August  month_December  \\\n",
       "0        234.0          241.0        158.0          95.0           142.0   \n",
       "1        137.0          147.0        114.0         114.0            34.0   \n",
       "2        197.0          289.0        117.0         133.0            91.0   \n",
       "3        179.0          206.0        112.0          67.0           134.0   \n",
       "4        814.0          885.0        709.0         465.0           290.0   \n",
       "\n",
       "    ...     month_November  month_October  month_September  time_1  time_2  \\\n",
       "0   ...               96.0          110.0            123.0   349.0    35.0   \n",
       "1   ...               53.0           65.0             96.0   271.0    38.0   \n",
       "2   ...               86.0          148.0            175.0   462.0    51.0   \n",
       "3   ...               23.0           68.0             68.0   157.0    27.0   \n",
       "4   ...              395.0          521.0            511.0  1295.0   137.0   \n",
       "\n",
       "   time_3  time_4  time_5  time_6  zipcode  \n",
       "0    57.0   206.0   437.0   663.0    10472  \n",
       "1    32.0   143.0   248.0   448.0    10037  \n",
       "2    56.0   190.0   411.0   483.0    10460  \n",
       "3    37.0    97.0   282.0   575.0    11224  \n",
       "4   110.0   662.0  1553.0  2340.0    10456  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data1=pd.read_csv(\"https://serv.cusp.nyu.edu/~lw1474/ADS_Data/session07/stop_2012.csv\")\n",
    "# data1=pd.concat((data1.zipcode,data1.iloc[:,:-1]),axis=1)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Q0. Get the feature space(All the columns except the last one:\"zipcode\") and save it as a new data set if you want\n",
    "\n",
    "#Q1. Get the total \"stop\"s for each zip code using two different timeline scales. (using pd.concate())\n",
    "\n",
    "#Q2. Find the most \"dangerous\" zip code by the total \"stop\"s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now if we want to cluster the zip code by the stops timeline data set. \n",
    "#(Don't forget to change your data set to numpy array form)\n",
    "\n",
    "#Q3.Choosing the number of clusters k for Kmeans. Try both elbow method and silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Q4. Using kmeans to cluster the zip codes.Choose a k as you wish from your elbow and SS result. And save your result as\n",
    "#the following dataframe/form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10037</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode  0\n",
       "0    10472  0\n",
       "1    10037  1\n",
       "2    10460  0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex=pd.DataFrame([[10472,10037,10460],[0,1,0]]).T\n",
    "ex.columns=[\"zipcode\",0]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Q5. Visuilize the result on the map by my function. Make sure your result has the right form\n",
    "#as in my example.\n",
    "\n",
    "# For this part you need to: \n",
    "#(1) Make sure that your result of Q4 is in the correct form.\n",
    "#(2) Make sure NY.shp, NY.shx, NY.dbf are downloaded in your working directory.\n",
    "#(3) Use the defined function plot_on_map(your_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import shapefile\n",
    "import shapefile as shp\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from descartes import PolygonPatch\n",
    "\n",
    "zipsf = shp.Reader('NY.shp')\n",
    "zipsf_records=zipsf.records()\n",
    "zipsf_shape=zipsf.shapeRecords()\n",
    "\n",
    "#for plotting our result\n",
    "def plot_on_map(result):\n",
    "    \n",
    "    x = {}\n",
    "    for i in range(len(result)):\n",
    "        key=result.loc[i,'zipcode']\n",
    "        x[key]=result.loc[i,0]\n",
    "\n",
    "    #x is a dictionary (zip codes vs values)\n",
    "    cmap = plt.cm.spectral(np.linspace(0,1,max(x.values())+1))\n",
    "\n",
    "    fig = plt.figure(figsize = (12,15)) \n",
    "    ax = fig.gca() \n",
    "    for s in range(0,len(zipsf_records)):\n",
    "        z=int(zipsf_records[s][0])\n",
    "        if z in x.keys(): \n",
    "            #from here\n",
    "            k = x[z] \n",
    "            c=cmap[k][0:3]    \n",
    "            shape=zipsf_shape[s]\n",
    "            x_ = [i[0] for i in shape.shape.points[:]]\n",
    "            y_ = [i[1] for i in shape.shape.points[:]]\n",
    "            poly=Polygon(zip(x_,y_))\n",
    "            #plt.fill(x,y,color=my_cmap[s])\n",
    "            ax.add_patch(PolygonPatch(poly, fc=c, ec='k', alpha=0.5, zorder=2 ))\n",
    "\n",
    "    ax.axis('scaled')\n",
    "    plt.title(\"clustering result on map\")\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "    clum_num=len(result.iloc[:,-1].unique())\n",
    "\n",
    "    handles=[]\n",
    "    for t in range(clum_num):\n",
    "        locals()[\"patch_{}\".format(t)] = mpatches.Patch(color=cmap[t][0:3] , label='cluster'+str(t+1))\n",
    "        handles.append(locals()[\"patch_{}\".format(t)])\n",
    "\n",
    "    plt.legend(handles=handles,loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Q6. Repeat Q4 and Q5 using Guassian Mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
